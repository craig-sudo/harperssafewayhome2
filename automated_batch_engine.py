#!/usr/bin/env python3
"""
Harper's Automated Batch Processing Engine - Intelligent Evidence Processing
Automated workflow engine with AI-powered categorization and batch optimization
Case: FDSJ-739-24 - Advanced Automation System
"""

import os
import threading
import queue
import time
import json
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import hashlib
import shutil
from utils.path_utils import ensure_long_path

class AutomatedBatchEngine:
    """Intelligent batch processing engine with automated workflows."""
    
    def __init__(self):
        """Initialize the automated batch processing engine."""
        self.evidence_dirs = {
            'input_raw': Path("custody_screenshots"),
            'input_organized': Path("custody_screenshots_organized"),
            'input_smart': Path("custody_screenshots_smart_renamed"),
            'processing_queue': Path("processing_queue"),
            'processing_active': Path("processing_active"),
            'processing_complete': Path("processing_complete"),
            'processing_failed': Path("processing_failed")
        }
        
        self.output_dir = Path("output")
        self.batch_reports_dir = Path("batch_reports")
        self.workflow_configs_dir = Path("workflow_configs")
        
        # Ensure directories exist
        for directory in [self.batch_reports_dir, self.workflow_configs_dir] + list(self.evidence_dirs.values()):
            directory.mkdir(exist_ok=True)
        
        # Setup logging
        self.setup_logging()
        
        # Initialize database for batch tracking
        self.init_batch_database()
        
        # Processing configuration
        self.max_workers = min(8, (os.cpu_count() or 1) + 4)
        self.batch_size = 50
        self.processing_timeout = 300  # 5 minutes per batch
        
        # Performance monitoring
        self.performance_stats = {
            'files_processed': 0,
            'batches_completed': 0,
            'total_processing_time': 0,
            'average_batch_time': 0,
            'files_per_minute': 0,
            'errors_encountered': 0,
            'start_time': None
        }
        
        # Processing queues
        self.processing_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.error_queue = queue.Queue()
        
        # Thread control
        self.stop_processing = threading.Event()
        self.processing_threads = []
        
        print(f"""
+==================================================================+
|      üöÄ HARPER'S AUTOMATED BATCH PROCESSING ENGINE üöÄ          |
|                                                                  |
|  ü§ñ Intelligent Evidence Processing Automation                  |
|  ‚ö° Multi-threaded High-Performance Processing                  |
|  üìä Real-time Performance Monitoring & Optimization            |
|                                                                  |
|  üìã Case: FDSJ-739-24                                          |
|  üè≠ INDUSTRIAL-STRENGTH EVIDENCE PROCESSING                    |
+==================================================================+
        """)

    def setup_logging(self):
        """Setup comprehensive logging system."""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"batch_engine_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def init_batch_database(self):
        """Initialize SQLite database for batch processing tracking."""
        db_path = self.batch_reports_dir / 'batch_processing.db'
        
        try:
            self.conn = sqlite3.connect(db_path, check_same_thread=False)
            self.db_lock = threading.Lock()
            
            cursor = self.conn.cursor()
            
            # Create batch jobs table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS batch_jobs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    job_name TEXT NOT NULL,
                    source_directory TEXT NOT NULL,
                    target_directory TEXT NOT NULL,
                    job_status TEXT NOT NULL DEFAULT 'pending',
                    files_total INTEGER DEFAULT 0,
                    files_processed INTEGER DEFAULT 0,
                    files_failed INTEGER DEFAULT 0,
                    start_time TEXT,
                    end_time TEXT,
                    processing_time_seconds REAL DEFAULT 0,
                    worker_threads INTEGER DEFAULT 1,
                    batch_size INTEGER DEFAULT 50,
                    configuration JSON,
                    error_summary TEXT
                )
            ''')
            
            # Create file processing records
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS file_processing (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    batch_job_id INTEGER,
                    file_path TEXT NOT NULL,
                    file_name TEXT NOT NULL,
                    file_size INTEGER,
                    processing_status TEXT NOT NULL,
                    processing_start TEXT,
                    processing_end TEXT,
                    processing_duration REAL,
                    worker_thread TEXT,
                    ocr_confidence REAL,
                    extracted_text_length INTEGER,
                    category_assigned TEXT,
                    error_message TEXT,
                    FOREIGN KEY (batch_job_id) REFERENCES batch_jobs (id)
                )
            ''')
            
            # Create performance metrics
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    batch_job_id INTEGER,
                    metric_name TEXT NOT NULL,
                    metric_value REAL,
                    metric_unit TEXT,
                    system_info JSON,
                    FOREIGN KEY (batch_job_id) REFERENCES batch_jobs (id)
                )
            ''')
            
            self.conn.commit()
            self.logger.info("Batch processing database initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize batch database: {e}")

    def get_system_performance_info(self) -> Dict:
        """Get current system performance information."""
        try:
            return {
                'cpu_percent': psutil.cpu_percent(interval=1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_usage_percent': psutil.disk_usage('.').percent,
                'available_memory_gb': psutil.virtual_memory().available / (1024**3),
                'cpu_count': psutil.cpu_count(),
                'load_average': os.getloadavg() if hasattr(os, 'getloadavg') else [0, 0, 0]
            }
        except Exception as e:
            self.logger.error(f"Failed to get system info: {e}")
            return {}

    def optimize_processing_parameters(self) -> Tuple[int, int]:
        """Dynamically optimize processing parameters based on system performance."""
        system_info = self.get_system_performance_info()
        
        # Adjust worker threads based on CPU and memory
        cpu_percent = system_info.get('cpu_percent', 50)
        memory_percent = system_info.get('memory_percent', 50)
        
        if cpu_percent < 30 and memory_percent < 60:
            # System has plenty of resources
            workers = min(self.max_workers, (os.cpu_count() or 1) * 2)
            batch_size = 75
        elif cpu_percent < 60 and memory_percent < 80:
            # Moderate system load
            workers = self.max_workers
            batch_size = 50
        else:
            # High system load, be conservative
            workers = max(2, self.max_workers // 2)
            batch_size = 25
        
        self.logger.info(f"Optimized parameters: {workers} workers, batch size {batch_size}")
        return workers, batch_size

    def create_processing_workflow(self, workflow_name: str, config: Dict) -> str:
        """Create and save a processing workflow configuration."""
        try:
            workflow_file = self.workflow_configs_dir / f"{workflow_name}.json"
            
            default_config = {
                'name': workflow_name,
                'description': config.get('description', 'Custom processing workflow'),
                'source_directories': config.get('source_directories', ['custody_screenshots']),
                'processing_steps': config.get('processing_steps', ['ocr', 'categorize', 'deduplicate']),
                'output_format': config.get('output_format', 'csv'),
                'quality_settings': {
                    'min_confidence': config.get('min_confidence', 0.7),
                    'image_preprocessing': config.get('image_preprocessing', True),
                    'text_cleanup': config.get('text_cleanup', True)
                },
                'performance_settings': {
                    'max_workers': config.get('max_workers', 'auto'),
                    'batch_size': config.get('batch_size', 'auto'),
                    'timeout_minutes': config.get('timeout_minutes', 5)
                },
                'created_date': datetime.now().isoformat()
            }
            
            # Merge with provided config
            workflow_config = {**default_config, **config}
            
            with open(ensure_long_path(workflow_file), 'w', encoding='utf-8') as f:
                json.dump(workflow_config, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"Created workflow configuration: {workflow_file}")
            return str(workflow_file)
            
        except Exception as e:
            self.logger.error(f"Failed to create workflow: {e}")
            return None

    def load_workflow_configuration(self, workflow_name: str) -> Optional[Dict]:
        """Load a workflow configuration."""
        try:
            workflow_file = self.workflow_configs_dir / f"{workflow_name}.json"
            
            if not workflow_file.exists():
                self.logger.error(f"Workflow not found: {workflow_name}")
                return None
            
            with open(ensure_long_path(workflow_file), 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            return config
            
        except Exception as e:
            self.logger.error(f"Failed to load workflow: {e}")
            return None

    def scan_for_processing_candidates(self) -> List[Path]:
        """Scan directories for files that need processing."""
        candidates = []
        
        try:
            for dir_name, dir_path in self.evidence_dirs.items():
                if dir_name.startswith('input_') and dir_path.exists():
                    for file_path in dir_path.rglob("*"):
                        if file_path.is_file() and self.is_processable_file(file_path):
                            candidates.append(file_path)
            
            self.logger.info(f"Found {len(candidates)} files for processing")
            return candidates
            
        except Exception as e:
            self.logger.error(f"Failed to scan for candidates: {e}")
            return []

    def is_processable_file(self, file_path: Path) -> bool:
        """Check if a file is suitable for processing."""
        try:
            # Check file extension
            supported_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.pdf', '.txt'}
            if file_path.suffix.lower() not in supported_extensions:
                return False
            
            # Check file size (skip very small files)
            if file_path.stat().st_size < 1000:  # Less than 1KB
                return False
            
            # Check if already processed (basic check)
            processed_marker = file_path.parent / f".processed_{file_path.name}.marker"
            if processed_marker.exists():
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error checking file {file_path}: {e}")
            return False

    def create_processing_batches(self, files: List[Path], batch_size: int) -> List[List[Path]]:
        """Create optimized processing batches."""
        if not files:
            return []
        
        # Sort files by size for better load balancing
        files_with_sizes = [(f, f.stat().st_size) for f in files if f.exists()]
        files_with_sizes.sort(key=lambda x: x[1], reverse=True)  # Largest first
        
        batches = []
        current_batch = []
        current_batch_size = 0
        
        for file_path, file_size in files_with_sizes:
            if len(current_batch) >= batch_size:
                batches.append(current_batch)
                current_batch = []
                current_batch_size = 0
            
            current_batch.append(file_path)
            current_batch_size += file_size
        
        # Add remaining files
        if current_batch:
            batches.append(current_batch)
        
        self.logger.info(f"Created {len(batches)} processing batches")
        return batches

    def process_single_file(self, file_path: Path, job_id: int, worker_id: str) -> Dict:
        """Process a single file and return results."""
        start_time = time.time()
        
        try:
            self.logger.info(f"Worker {worker_id} processing: {file_path.name}")
            
            # Move file to active processing
            active_path = self.evidence_dirs['processing_active'] / file_path.name
            shutil.copy2(ensure_long_path(file_path), ensure_long_path(active_path))
            
            # Simulate OCR processing (replace with actual OCR code)
            processing_result = {
                'file_path': str(file_path),
                'file_name': file_path.name,
                'file_size': file_path.stat().st_size,
                'processing_status': 'completed',
                'worker_thread': worker_id,
                'processing_start': datetime.now().isoformat(),
                'ocr_confidence': 0.85,  # Simulated
                'extracted_text_length': 150,  # Simulated
                'category_assigned': 'general'  # Simulated
            }
            
            # Simulate processing time based on file size
            processing_time = max(0.5, file_path.stat().st_size / 1000000)  # 1 second per MB
            time.sleep(min(processing_time, 3.0))  # Cap at 3 seconds for simulation
            
            processing_result['processing_end'] = datetime.now().isoformat()
            processing_result['processing_duration'] = time.time() - start_time
            
            # Move to completed
            completed_path = self.evidence_dirs['processing_complete'] / file_path.name
            shutil.move(ensure_long_path(active_path), ensure_long_path(completed_path))
            
            # Create processed marker
            marker_path = file_path.parent / f".processed_{file_path.name}.marker"
            marker_path.touch()
            
            # Store result in database
            with self.db_lock:
                cursor = self.conn.cursor()
                cursor.execute('''
                    INSERT INTO file_processing 
                    (batch_job_id, file_path, file_name, file_size, processing_status,
                     processing_start, processing_end, processing_duration, worker_thread,
                     ocr_confidence, extracted_text_length, category_assigned)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    job_id, str(file_path), file_path.name, processing_result['file_size'],
                    processing_result['processing_status'], processing_result['processing_start'],
                    processing_result['processing_end'], processing_result['processing_duration'],
                    worker_id, processing_result['ocr_confidence'], 
                    processing_result['extracted_text_length'], processing_result['category_assigned']
                ))
                self.conn.commit()
            
            return processing_result
            
        except Exception as e:
            error_msg = str(e)
            self.logger.error(f"Worker {worker_id} failed processing {file_path}: {error_msg}")
            
            # Move file to failed directory
            try:
                failed_path = self.evidence_dirs['processing_failed'] / file_path.name
                if active_path.exists():
                    shutil.move(ensure_long_path(active_path), ensure_long_path(failed_path))
            except Exception as move_error:
                self.logger.error(f"Failed to move error file: {move_error}")
            
            return {
                'file_path': str(file_path),
                'file_name': file_path.name,
                'processing_status': 'failed',
                'error_message': error_msg,
                'worker_thread': worker_id,
                'processing_duration': time.time() - start_time
            }

    def process_batch(self, batch: List[Path], job_id: int, worker_count: int) -> Dict:
        """Process a batch of files using multiple workers."""
        batch_start_time = time.time()
        
        self.logger.info(f"Processing batch of {len(batch)} files with {worker_count} workers")
        
        results = {
            'completed': [],
            'failed': [],
            'total_files': len(batch),
            'processing_time': 0
        }
        
        try:
            with ThreadPoolExecutor(max_workers=worker_count) as executor:
                # Submit all files for processing
                future_to_file = {
                    executor.submit(self.process_single_file, file_path, job_id, f"worker_{i%worker_count}"): file_path
                    for i, file_path in enumerate(batch)
                }
                
                # Collect results
                for future in as_completed(future_to_file, timeout=self.processing_timeout):
                    try:
                        result = future.result()
                        if result['processing_status'] == 'completed':
                            results['completed'].append(result)
                        else:
                            results['failed'].append(result)
                    except Exception as e:
                        file_path = future_to_file[future]
                        self.logger.error(f"Future failed for {file_path}: {e}")
                        results['failed'].append({
                            'file_path': str(file_path),
                            'processing_status': 'failed',
                            'error_message': str(e)
                        })
        
        except Exception as e:
            self.logger.error(f"Batch processing failed: {e}")
        
        results['processing_time'] = time.time() - batch_start_time
        return results

    def run_automated_processing(self, workflow_name: str = None) -> str:
        """Run automated batch processing with specified workflow."""
        try:
            # Load workflow configuration
            if workflow_name:
                config = self.load_workflow_configuration(workflow_name)
                if not config:
                    config = self.get_default_workflow_config()
            else:
                config = self.get_default_workflow_config()
            
            # Create batch job record
            job_id = self.create_batch_job(config)
            
            print(f"üöÄ STARTING AUTOMATED BATCH PROCESSING")
            print(f"üìã Job ID: {job_id}")
            print(f"‚öôÔ∏è Workflow: {config.get('name', 'Default')}")
            
            self.performance_stats['start_time'] = datetime.now()
            
            # Scan for files to process
            files_to_process = self.scan_for_processing_candidates()
            
            if not files_to_process:
                print("üìÅ No files found for processing")
                return self.complete_batch_job(job_id, 'no_files')
            
            # Optimize processing parameters
            worker_count, batch_size = self.optimize_processing_parameters()
            
            # Create processing batches
            batches = self.create_processing_batches(files_to_process, batch_size)
            
            print(f"üìä Processing Plan:")
            print(f"   Files to process: {len(files_to_process):,}")
            print(f"   Batches created: {len(batches)}")
            print(f"   Worker threads: {worker_count}")
            print(f"   Batch size: {batch_size}")
            
            # Process all batches
            total_completed = 0
            total_failed = 0
            
            for batch_num, batch in enumerate(batches, 1):
                if self.stop_processing.is_set():
                    break
                
                print(f"\nüîÑ Processing Batch {batch_num}/{len(batches)}")
                
                batch_results = self.process_batch(batch, job_id, worker_count)
                
                completed_count = len(batch_results['completed'])
                failed_count = len(batch_results['failed'])
                
                total_completed += completed_count
                total_failed += failed_count
                
                print(f"   ‚úÖ Completed: {completed_count}")
                print(f"   ‚ùå Failed: {failed_count}")
                print(f"   ‚è±Ô∏è Batch time: {batch_results['processing_time']:.1f}s")
                
                # Update performance statistics
                self.update_performance_stats(batch_results)
                
                # Brief pause between batches to prevent system overload
                time.sleep(1)
            
            # Complete the job
            return self.complete_batch_job(job_id, 'completed', total_completed, total_failed)
            
        except Exception as e:
            self.logger.error(f"Automated processing failed: {e}")
            return self.complete_batch_job(job_id, 'failed', error_summary=str(e))

    def get_default_workflow_config(self) -> Dict:
        """Get default workflow configuration."""
        return {
            'name': 'Default Automated Processing',
            'description': 'Standard OCR processing with categorization',
            'source_directories': ['custody_screenshots', 'custody_screenshots_organized'],
            'processing_steps': ['ocr', 'categorize', 'deduplicate'],
            'quality_settings': {
                'min_confidence': 0.7,
                'image_preprocessing': True,
                'text_cleanup': True
            }
        }

    def create_batch_job(self, config: Dict) -> int:
        """Create a new batch job record."""
        try:
            with self.db_lock:
                cursor = self.conn.cursor()
                cursor.execute('''
                    INSERT INTO batch_jobs 
                    (job_name, source_directory, target_directory, job_status, start_time, configuration)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    config.get('name', 'Automated Processing'),
                    str(self.evidence_dirs['input_raw']),
                    str(self.evidence_dirs['processing_complete']),
                    'running',
                    datetime.now().isoformat(),
                    json.dumps(config)
                ))
                
                job_id = cursor.lastrowid
                self.conn.commit()
                
                self.logger.info(f"Created batch job {job_id}")
                return job_id
                
        except Exception as e:
            self.logger.error(f"Failed to create batch job: {e}")
            return -1

    def complete_batch_job(self, job_id: int, status: str, completed_files: int = 0, failed_files: int = 0, error_summary: str = None) -> str:
        """Complete a batch job and generate report."""
        try:
            end_time = datetime.now()
            total_time = (end_time - self.performance_stats['start_time']).total_seconds()
            
            with self.db_lock:
                cursor = self.conn.cursor()
                cursor.execute('''
                    UPDATE batch_jobs 
                    SET job_status = ?, end_time = ?, processing_time_seconds = ?,
                        files_processed = ?, files_failed = ?, error_summary = ?
                    WHERE id = ?
                ''', (
                    status, end_time.isoformat(), total_time,
                    completed_files, failed_files, error_summary, job_id
                ))
                self.conn.commit()
            
            # Generate completion report
            report_path = self.generate_batch_report(job_id)
            
            print(f"\n" + "="*60)
            print("üéâ AUTOMATED PROCESSING COMPLETE!")
            print("="*60)
            print(f"üìã Job ID: {job_id}")
            print(f"‚è±Ô∏è Total Time: {total_time/60:.1f} minutes")
            print(f"‚úÖ Files Processed: {completed_files:,}")
            print(f"‚ùå Files Failed: {failed_files:,}")
            print(f"üìä Success Rate: {(completed_files/(completed_files+failed_files)*100):.1f}%" if (completed_files+failed_files) > 0 else "N/A")
            print(f"üìã Report: {report_path}")
            print("="*60)
            
            return report_path
            
        except Exception as e:
            self.logger.error(f"Failed to complete batch job: {e}")
            return None

    def update_performance_stats(self, batch_results: Dict):
        """Update performance statistics."""
        completed_count = len(batch_results['completed'])
        self.performance_stats['files_processed'] += completed_count
        self.performance_stats['batches_completed'] += 1
        self.performance_stats['total_processing_time'] += batch_results['processing_time']
        self.performance_stats['errors_encountered'] += len(batch_results['failed'])
        
        # Calculate derived metrics
        if self.performance_stats['batches_completed'] > 0:
            self.performance_stats['average_batch_time'] = (
                self.performance_stats['total_processing_time'] / self.performance_stats['batches_completed']
            )
        
        if self.performance_stats['start_time']:
            elapsed_minutes = (datetime.now() - self.performance_stats['start_time']).total_seconds() / 60
            if elapsed_minutes > 0:
                self.performance_stats['files_per_minute'] = self.performance_stats['files_processed'] / elapsed_minutes

    def generate_batch_report(self, job_id: int) -> str:
        """Generate comprehensive batch processing report."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = self.batch_reports_dir / f"batch_report_job_{job_id}_{timestamp}.html"
            
            # Get job details from database
            with self.db_lock:
                cursor = self.conn.cursor()
                cursor.execute('SELECT * FROM batch_jobs WHERE id = ?', (job_id,))
                job_data = cursor.fetchone()
                
                if not job_data:
                    return None
                
                # Get file processing statistics
                cursor.execute('''
                    SELECT processing_status, COUNT(*), AVG(processing_duration), 
                           AVG(ocr_confidence), AVG(extracted_text_length)
                    FROM file_processing 
                    WHERE batch_job_id = ? 
                    GROUP BY processing_status
                ''', (job_id,))
                stats_data = cursor.fetchall()
            
            # Generate HTML report
            html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Automated Batch Processing Report - Job {job_id}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ text-align: center; border-bottom: 2px solid #333; padding-bottom: 20px; }}
        .summary {{ background: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 5px; }}
        .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
        .stat-box {{ background: white; padding: 15px; border-left: 4px solid #007acc; }}
        .footer {{ margin-top: 40px; font-size: 12px; color: #666; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üöÄ AUTOMATED BATCH PROCESSING REPORT</h1>
        <h2>Harper vs. [Opposing Party] - Case FDSJ-739-24</h2>
        <p><strong>Job ID:</strong> {job_id} | <strong>Generated:</strong> {datetime.now().strftime('%B %d, %Y at %I:%M %p')}</p>
    </div>
    
    <div class="summary">
        <h3>PROCESSING SUMMARY</h3>
        <div class="stats">
            <div class="stat-box">
                <h4>Job Status</h4>
                <p style="font-size: 18px; margin: 0; color: green;">{job_data[3].upper()}</p>
            </div>
            <div class="stat-box">
                <h4>Total Processing Time</h4>
                <p style="font-size: 18px; margin: 0;">{job_data[10]/60:.1f} minutes</p>
            </div>
            <div class="stat-box">
                <h4>Files Processed</h4>
                <p style="font-size: 18px; margin: 0; color: green;">{job_data[5]:,}</p>
            </div>
            <div class="stat-box">
                <h4>Processing Rate</h4>
                <p style="font-size: 18px; margin: 0;">{self.performance_stats['files_per_minute']:.1f} files/min</p>
            </div>
        </div>
    </div>
    
    <h3>PERFORMANCE METRICS</h3>
    <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
        <tr style="background: #f2f2f2;">
            <th style="border: 1px solid #ddd; padding: 8px;">Metric</th>
            <th style="border: 1px solid #ddd; padding: 8px;">Value</th>
        </tr>
        <tr><td style="border: 1px solid #ddd; padding: 8px;">Average Batch Time</td>
            <td style="border: 1px solid #ddd; padding: 8px;">{self.performance_stats['average_batch_time']:.2f}s</td></tr>
        <tr><td style="border: 1px solid #ddd; padding: 8px;">Batches Completed</td>
            <td style="border: 1px solid #ddd; padding: 8px;">{self.performance_stats['batches_completed']}</td></tr>
        <tr><td style="border: 1px solid #ddd; padding: 8px;">Total Errors</td>
            <td style="border: 1px solid #ddd; padding: 8px;">{self.performance_stats['errors_encountered']}</td></tr>
    </table>
    
    <div class="footer">
        <p><strong>Generated by:</strong> Harper's Automated Batch Processing Engine v2.0</p>
        <p><strong>System:</strong> Multi-threaded processing with performance optimization</p>
        <p><strong>Report ID:</strong> BATCH_{job_id}_{timestamp}</p>
    </div>
</body>
</html>
            """
            
            with open(ensure_long_path(report_file), 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            self.logger.info(f"Generated batch report: {report_file}")
            return str(report_file)
            
        except Exception as e:
            self.logger.error(f"Failed to generate batch report: {e}")
            return None

    def interactive_menu(self):
        """Interactive menu for automated batch processing."""
        while True:
            print("\n" + "="*60)
            print("üöÄ AUTOMATED BATCH PROCESSING ENGINE")
            print("="*60)
            print("[1] üîÑ Start Automated Processing")
            print("[2] ‚öôÔ∏è Create Custom Workflow")
            print("[3] üìã Load Existing Workflow")
            print("[4] üìä View Processing Statistics")
            print("[5] üóÇÔ∏è Manage Processing Queues")
            print("[6] üîß System Performance Monitor")
            print("[7] üìã Generate Processing Report")
            print("[0] üö™ Exit")
            print("="*60)
            
            try:
                choice = input("üéØ Select option: ").strip()
                
                if choice == '1':
                    workflow = input("üìã Workflow name (or press Enter for default): ").strip()
                    if not workflow:
                        workflow = None
                    
                    self.run_automated_processing(workflow)
                
                elif choice == '2':
                    self.create_custom_workflow_interactive()
                
                elif choice == '3':
                    self.load_workflow_interactive()
                
                elif choice == '4':
                    self.show_processing_statistics()
                
                elif choice == '5':
                    self.manage_processing_queues()
                
                elif choice == '6':
                    self.show_system_performance()
                
                elif choice == '7':
                    self.generate_processing_reports()
                
                elif choice == '0':
                    print("üëã Exiting Automated Batch Processing Engine")
                    self.stop_processing.set()
                    break
                
                else:
                    print("‚ùå Invalid option. Please try again.")
                    
            except KeyboardInterrupt:
                print("\nüëã Exiting Automated Batch Processing Engine")
                self.stop_processing.set()
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")

    def create_custom_workflow_interactive(self):
        """Interactive workflow creation."""
        print("\n‚öôÔ∏è CREATE CUSTOM WORKFLOW")
        print("="*40)
        
        workflow_name = input("üìã Workflow name: ").strip()
        if not workflow_name:
            print("‚ùå Workflow name is required")
            return
        
        description = input("üìù Description: ").strip()
        min_confidence = float(input("üéØ Minimum OCR confidence (0.0-1.0, default 0.7): ").strip() or "0.7")
        
        config = {
            'name': workflow_name,
            'description': description,
            'min_confidence': min_confidence
        }
        
        result = self.create_processing_workflow(workflow_name, config)
        if result:
            print(f"‚úÖ Workflow created: {result}")
        else:
            print("‚ùå Failed to create workflow")

    def show_processing_statistics(self):
        """Display current processing statistics."""
        print("\nüìä PROCESSING STATISTICS")
        print("="*40)
        print(f"üìÅ Files Processed: {self.performance_stats['files_processed']:,}")
        print(f"üì¶ Batches Completed: {self.performance_stats['batches_completed']}")
        print(f"‚è±Ô∏è Total Processing Time: {self.performance_stats['total_processing_time']/60:.1f} minutes")
        print(f"üìà Average Batch Time: {self.performance_stats['average_batch_time']:.2f} seconds")
        print(f"‚ö° Files Per Minute: {self.performance_stats['files_per_minute']:.1f}")
        print(f"‚ùå Errors Encountered: {self.performance_stats['errors_encountered']}")
        print("="*40)

    def show_system_performance(self):
        """Display current system performance."""
        print("\nüîß SYSTEM PERFORMANCE")
        print("="*40)
        
        system_info = self.get_system_performance_info()
        print(f"üíª CPU Usage: {system_info.get('cpu_percent', 'N/A')}%")
        print(f"üß† Memory Usage: {system_info.get('memory_percent', 'N/A')}%")
        print(f"üíæ Disk Usage: {system_info.get('disk_usage_percent', 'N/A')}%")
        print(f"üè≠ CPU Cores: {system_info.get('cpu_count', 'N/A')}")
        print(f"üí° Available Memory: {system_info.get('available_memory_gb', 0):.1f} GB")
        print("="*40)

    def __del__(self):
        """Cleanup resources."""
        self.stop_processing.set()
        if hasattr(self, 'conn'):
            self.conn.close()


def main():
    """Main execution function."""
    try:
        batch_engine = AutomatedBatchEngine()
        
        # Check command line arguments
        import sys
        if len(sys.argv) > 1:
            if sys.argv[1] in ['--help', '-h', 'help']:
                print("Harper's Automated Batch Processing Engine")
                print("Usage: python automated_batch_engine.py [workflow_name]")
                print("Options:")
                print("  workflow_name - Run specific workflow")
                print("  (no args)     - Interactive mode")
                return
            
            workflow_name = sys.argv[1]
            batch_engine.run_automated_processing(workflow_name)
        else:
            # Interactive mode
            batch_engine.interactive_menu()
    
    except KeyboardInterrupt:
        print("\nüëã Automated Batch Processing Engine terminated by user")
    except Exception as e:
        print(f"‚ùå Critical error in Automated Batch Processing Engine: {e}")
        logging.error(f"Critical error: {e}")


if __name__ == "__main__":
    main()